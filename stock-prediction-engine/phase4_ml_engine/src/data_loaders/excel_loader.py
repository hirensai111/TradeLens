#!/usr/bin/env python3
"""
Enhanced Excel Loader - Load ALL valuable sheets from your Excel file
Includes Summary, Company Info, Data Quality, and Metadata
FIXED VERSION - Correctly handles performance metrics and provides detailed debugging
"""
import pandas as pd
import numpy as np
import yaml
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

class ExcelDataLoader:
    """
    Enhanced Excel data loader that processes ALL sheets in your analysis file
    Extracts maximum value from Summary, Company Info, Data Quality, etc.
    FIXED VERSION with proper performance metrics handling
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """Initialize enhanced Excel data loader"""
        self.config = self._load_config(config_path)
        self.excel_path = self.config['data_sources']['excel_path']
        self.logger = self._setup_logging()
        
        # Data storage for all sheets
        self.raw_data = None
        self.technical_data = None
        self.sentiment_data = None
        self.performance_data = None
        self.summary_data = None
        self.company_info = None
        self.data_quality = None
        self.metadata = None
        self.price_charts_info = None
        
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration with fallback"""
        try:
            with open(config_path, 'r') as file:
                return yaml.safe_load(file)
        except Exception as e:
            print(f"Warning: Could not load config {config_path}: {e}")
            return self._default_config()
    
    def _default_config(self) -> Dict:
        """Enhanced default configuration"""
        return {
            'data_sources': {
                'excel_path': 'D:/stock-prediction-engine/phase4_ml_engine/data/TEM_analysis_report_20250715.xlsx'
            },
            'features': {
                'technical_indicators': ['SMA_20', 'SMA_50', 'SMA_200', 'RSI', 'MACD', 'MACD_Signal', 'BB_Upper', 'BB_Lower', 'ATR'],
                'fundamental_metrics': ['market_cap', 'pe_ratio', 'dividend_yield', 'debt_to_equity'],
                'quality_thresholds': {'min_data_completeness': 0.95, 'max_missing_days': 5}
            }
        }
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging"""
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        return logging.getLogger(__name__)
    
    def load_all_data(self) -> Dict[str, pd.DataFrame]:
        """Load ALL sheets from Excel file - comprehensive extraction"""
        self.logger.info(f"Loading comprehensive Excel file: {self.excel_path}")
        
        try:
            # Load all sheets
            excel_data = pd.read_excel(self.excel_path, sheet_name=None)
            self.logger.info(f"Found {len(excel_data)} sheets: {list(excel_data.keys())}")
            
            processed_data = {}
            
            # Process each sheet based on its name
            for sheet_name, sheet_df in excel_data.items():
                self.logger.info(f"Processing sheet: {sheet_name}")
                
                if sheet_name == "Summary":
                    processed_data['summary_data'] = self._process_summary_data(sheet_df)
                    self.summary_data = processed_data['summary_data']
                    
                elif sheet_name == "Company Info":
                    processed_data['company_info'] = self._process_company_info(sheet_df)
                    self.company_info = processed_data['company_info']
                    
                elif sheet_name == "Price Charts":
                    processed_data['price_charts_info'] = self._process_price_charts(sheet_df)
                    self.price_charts_info = processed_data['price_charts_info']
                    
                elif sheet_name == "Technical Analysis":
                    processed_data['technical_data'] = self._process_technical_analysis(sheet_df)
                    self.technical_data = processed_data['technical_data']
                    
                elif sheet_name == "Sentiment Analysis":
                    processed_data['sentiment_data'] = self._process_sentiment_analysis(sheet_df)
                    self.sentiment_data = processed_data['sentiment_data']
                    
                elif sheet_name == "Performance Metrics":
                    processed_data['performance_data'] = self._process_performance_metrics(sheet_df)
                    self.performance_data = processed_data['performance_data']
                    
                elif sheet_name == "Raw Data":
                    processed_data['raw_data'] = self._process_raw_data_sheet(sheet_df)
                    self.raw_data = processed_data['raw_data']
                    
                elif sheet_name == "Data Quality":
                    processed_data['data_quality'] = self._process_data_quality_sheet(sheet_df)
                    self.data_quality = processed_data['data_quality']
                    
                elif sheet_name == "Metadata":
                    processed_data['metadata'] = self._process_metadata_sheet(sheet_df)
                    self.metadata = processed_data['metadata']
                else:
                    self.logger.warning(f"Unknown sheet: {sheet_name}")
            
            self.logger.info(f"Successfully processed {len(processed_data)} sheets")
            return processed_data
            
        except Exception as e:
            self.logger.error(f"Failed to load Excel file: {e}")
            raise
    
    def _process_summary_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Summary sheet - extract key insights and metrics"""
        self.logger.info("Processing Summary sheet")
        summary_metrics = {}
        
        try:
            # Skip title rows
            data_start = 4  # Data starts after title and headers
            
            # Scan through the sheet to extract key-value pairs
            for idx in range(data_start, len(df)):
                row = df.iloc[idx]
                
                # Extract key-value pairs from columns A and B
                if pd.notna(row.iloc[0]) and len(row) > 1 and pd.notna(row.iloc[1]):
                    key = str(row.iloc[0]).strip()
                    value = row.iloc[1]
                    
                    # Clean up the key
                    clean_key = key.replace(':', '').replace(' ', '_').lower()
                    
                    # Store numeric values
                    try:
                        if isinstance(value, str) and '%' in value:
                            summary_metrics[clean_key] = float(value.replace('%', '').replace(',', ''))
                        elif isinstance(value, str) and '$' in value:
                            summary_metrics[clean_key] = float(value.replace('$', '').replace(',', ''))
                        elif isinstance(value, (int, float)):
                            summary_metrics[clean_key] = float(value)
                        else:
                            summary_metrics[clean_key] = str(value)
                    except:
                        summary_metrics[clean_key] = str(value)
                
                # Also check columns E and F for additional metrics
                if len(row) > 5 and pd.notna(row.iloc[4]) and pd.notna(row.iloc[5]):
                    key = str(row.iloc[4]).strip()
                    value = row.iloc[5]
                    
                    clean_key = key.replace(':', '').replace(' ', '_').lower()
                    
                    try:
                        if isinstance(value, str) and '%' in value:
                            summary_metrics[clean_key] = float(value.replace('%', '').replace(',', ''))
                        elif isinstance(value, str) and '$' in value:
                            summary_metrics[clean_key] = float(value.replace('$', '').replace(',', ''))
                        elif isinstance(value, (int, float)):
                            summary_metrics[clean_key] = float(value)
                        else:
                            summary_metrics[clean_key] = str(value)
                    except:
                        summary_metrics[clean_key] = str(value)
            
            self.logger.info(f"Extracted {len(summary_metrics)} metrics from Summary sheet")
            return pd.DataFrame([summary_metrics]) if summary_metrics else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Summary sheet: {e}")
            return pd.DataFrame()
    
    def _process_company_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Company Info sheet - extract fundamental data"""
        self.logger.info("Processing Company Info sheet")
        company_data = {}
        
        try:
            # Scan through the sheet for key-value pairs
            for idx in range(len(df)):
                row = df.iloc[idx]
                
                # Check columns A and B
                if pd.notna(row.iloc[0]) and len(row) > 1 and pd.notna(row.iloc[1]):
                    key = str(row.iloc[0]).strip()
                    value = row.iloc[1]
                    
                    # Skip section headers
                    if key in ['Basic Information', 'Business Description', 'Financial Metrics', 
                               'Valuation Ratios', 'Dividend Information']:
                        continue
                    
                    clean_key = key.replace(' ', '_').replace(':', '').lower()
                    
                    # Convert values appropriately
                    try:
                        if isinstance(value, str):
                            if '$' in value and ('B' in value or 'M' in value or 'T' in value):
                                # Handle market cap format
                                company_data[clean_key] = value
                            elif '%' in value:
                                company_data[clean_key] = float(value.replace('%', ''))
                            elif value.replace('.', '').replace('-', '').isdigit():
                                company_data[clean_key] = float(value)
                            else:
                                company_data[clean_key] = value
                        else:
                            company_data[clean_key] = value
                    except:
                        company_data[clean_key] = str(value)
                
                # Also check columns D and E for financial metrics
                if len(row) > 4 and pd.notna(row.iloc[3]) and pd.notna(row.iloc[4]):
                    key = str(row.iloc[3]).strip()
                    value = row.iloc[4]
                    
                    if key not in ['', 'nan'] and not any(header in key for header in 
                                                          ['Basic Information', 'Financial Metrics']):
                        clean_key = key.replace(' ', '_').replace(':', '').lower()
                        try:
                            if isinstance(value, str) and '%' in value:
                                company_data[clean_key] = float(value.replace('%', ''))
                            elif isinstance(value, (int, float)):
                                company_data[clean_key] = float(value)
                            else:
                                company_data[clean_key] = str(value)
                        except:
                            company_data[clean_key] = str(value)
            
            self.logger.info(f"Extracted {len(company_data)} company info fields")
            return pd.DataFrame([company_data]) if company_data else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Company Info sheet: {e}")
            return pd.DataFrame()
    
    def _process_price_charts(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Price Charts sheet - extract chart information"""
        self.logger.info("Processing Price Charts sheet")
        chart_info = {}
        
        try:
            # This sheet mainly contains images, but we can extract text information
            for idx in range(len(df)):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]):
                    text = str(row.iloc[0])
                    
                    if 'data range' in text.lower():
                        chart_info['data_range'] = text
                    elif 'note:' in text.lower():
                        chart_info['chart_note'] = text
                    elif text.strip() and len(text) > 10:
                        # Store any other significant text
                        chart_info[f'info_{idx}'] = text
            
            return pd.DataFrame([chart_info]) if chart_info else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Price Charts sheet: {e}")
            return pd.DataFrame()
    
    def _process_technical_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Technical Analysis sheet with PROPER volatility calculation"""
        self.logger.info("Processing Technical Analysis sheet")
        
        try:
            # Find the header row (contains 'Date', 'Close', etc.)
            header_row = None
            for idx in range(min(10, len(df))):
                row = df.iloc[idx]
                if 'Date' in str(row.iloc[0]) or (pd.notna(row.iloc[0]) and 'Date' in str(row.iloc[0])):
                    header_row = idx
                    break
            
            if header_row is None:
                self.logger.warning("Could not find header row in Technical Analysis sheet")
                return pd.DataFrame()
            
            # Extract data starting from header row
            tech_df = df.iloc[header_row:].copy()
            tech_df.columns = tech_df.iloc[0]
            tech_df = tech_df.iloc[1:].reset_index(drop=True)
            
            # Clean column names
            tech_df.columns = [str(col).strip() for col in tech_df.columns]
            
            # Convert Date column
            if 'Date' in tech_df.columns:
                tech_df['Date'] = pd.to_datetime(tech_df['Date'], errors='coerce')
                tech_df = tech_df.dropna(subset=['Date'])
                tech_df.set_index('Date', inplace=True)
            
            # Convert numeric columns
            numeric_columns = ['Close', 'Volume', 'SMA_20', 'SMA_50', 'SMA_200', 
                            'RSI', 'MACD', 'MACD_Signal', 'BB_Upper', 'BB_Lower', 'ATR']
            
            for col in numeric_columns:
                if col in tech_df.columns:
                    tech_df[col] = pd.to_numeric(tech_df[col], errors='coerce')
            
            # Sort by date
            tech_df = tech_df.sort_index()
            
            # *** FIXED: PROPER VOLATILITY CALCULATION ***
            if 'Close' in tech_df.columns and len(tech_df) > 1:
                # Remove any rows with missing Close prices
                tech_df_clean = tech_df.dropna(subset=['Close'])
                
                if len(tech_df_clean) > 10:  # Need at least 10 data points
                    # Calculate daily returns (percentage change)
                    daily_returns = tech_df_clean['Close'].pct_change().dropna()
                    
                    # Debug: Check the returns
                    self.logger.info(f"Daily returns sample: {daily_returns.head().tolist()}")
                    self.logger.info(f"Daily returns stats: mean={daily_returns.mean():.6f}, std={daily_returns.std():.6f}")
                    
                    if len(daily_returns) > 5:
                        # Calculate daily volatility (standard deviation of returns)
                        daily_volatility = daily_returns.std()
                        
                        # CRITICAL FIX: Proper annualization
                        # Standard method: multiply by sqrt(252) where 252 = trading days per year
                        annualized_volatility = daily_volatility * np.sqrt(252)
                        
                        # Convert to percentage
                        volatility_percentage = annualized_volatility * 100
                        
                        # SANITY CHECK: Typical stock volatilities are 15-40%
                        if volatility_percentage > 200:  # Clearly wrong
                            self.logger.warning(f"Calculated volatility seems too high: {volatility_percentage:.2f}%")
                            
                            # Alternative calculation: maybe the data is already in percentage form
                            # Try treating Close prices as already percentage changes
                            alt_daily_returns = tech_df_clean['Close'].diff() / tech_df_clean['Close'].shift(1)
                            alt_daily_returns = alt_daily_returns.dropna()
                            
                            if len(alt_daily_returns) > 5:
                                alt_daily_volatility = alt_daily_returns.std()
                                alt_annualized_volatility = alt_daily_volatility * np.sqrt(252) * 100
                                
                                if 10 <= alt_annualized_volatility <= 100:  # More reasonable range
                                    volatility_percentage = alt_annualized_volatility
                                    self.logger.info(f"Using alternative volatility calculation: {volatility_percentage:.2f}%")
                        
                        # Final sanity check and fallback
                        if volatility_percentage > 150 or volatility_percentage < 5:
                            self.logger.warning(f"Volatility {volatility_percentage:.2f}% seems unrealistic, using fallback")
                            volatility_percentage = 25.0  # Reasonable default
                        
                        self.calculated_volatility = volatility_percentage
                        
                        self.logger.info(f"CORRECTED VOLATILITY CALCULATION:")
                        self.logger.info(f"   Number of data points: {len(daily_returns)}")
                        self.logger.info(f"   Daily volatility: {daily_volatility:.6f}")
                        self.logger.info(f"   Annualized volatility: {annualized_volatility:.6f}")
                        self.logger.info(f"   Final volatility %: {volatility_percentage:.2f}%")
                        
                        # Add volatility metrics to the dataframe
                        tech_df['Daily_Volatility'] = daily_volatility
                        tech_df['Annualized_Volatility'] = annualized_volatility
                        
                    else:
                        self.logger.warning(f"Not enough clean returns for volatility: {len(daily_returns)}")
                        self.calculated_volatility = 25.0
                else:
                    self.logger.warning(f"Not enough clean data for volatility: {len(tech_df_clean)}")
                    self.calculated_volatility = 25.0
            else:
                self.logger.warning("No Close price data found for volatility calculation")
                self.calculated_volatility = 25.0
            
            self.logger.info(f"Processed technical data with {len(tech_df)} rows and {len(tech_df.columns)} columns")
            return tech_df
            
        except Exception as e:
            self.logger.error(f"Error processing Technical Analysis sheet: {e}")
            self.calculated_volatility = 25.0
            return pd.DataFrame()

    # Additional method to check volatility from Performance Metrics sheet
    def get_volatility_from_performance_sheet(self) -> float:
        """Try to get volatility directly from Performance Metrics sheet"""
        if self.performance_data is not None and not self.performance_data.empty:
            perf_row = self.performance_data.iloc[0]
            
            # Look for volatility in performance metrics
            volatility_keys = ['annual_volatility', 'volatility', 'performance_annual_volatility']
            
            for key in volatility_keys:
                if key in perf_row.index and pd.notna(perf_row[key]):
                    vol_value = float(perf_row[key])
                    
                    # Check if it's reasonable (10% to 80% is typical for stocks)
                    if 5 <= vol_value <= 100:
                        self.logger.info(f"Found volatility in Performance sheet: {vol_value:.2f}%")
                        return vol_value
        
        return None

    def get_proper_volatility(self) -> float:
        """Get volatility with multiple fallback methods"""
        # Method 1: Try from Performance Metrics sheet first
        perf_volatility = self.get_volatility_from_performance_sheet()
        if perf_volatility is not None:
            return perf_volatility
        
        # Method 2: Use calculated volatility from technical analysis
        if hasattr(self, 'calculated_volatility'):
            if 5 <= self.calculated_volatility <= 100:  # Sanity check
                return self.calculated_volatility
        
        # Method 3: Fallback to reasonable default
        self.logger.warning("Using default volatility of 25%")
        return 25.0
    
    def _process_sentiment_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Sentiment Analysis sheet with event breakdown"""
        self.logger.info("Processing Sentiment Analysis sheet")
        
        try:
            # First, extract summary statistics
            sentiment_summary = {}
            
            # Look for summary section
            for idx in range(min(30, len(df))):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and len(row) > 1:
                    key = str(row.iloc[0]).strip()
                    
                    # Extract summary statistics
                    if any(term in key for term in ['Total Analyzed Events', 'Bullish Events', 
                                                    'Bearish Events', 'Neutral Events', 
                                                    'High Confidence Events', 'Average']):
                        if pd.notna(row.iloc[1]):
                            value = row.iloc[1]
                            clean_key = key.replace(':', '').replace(' ', '_').lower()
                            
                            try:
                                if isinstance(value, str) and '(' in value:
                                    # Extract the number before parentheses
                                    num_str = value.split('(')[0].strip()
                                    sentiment_summary[clean_key] = int(num_str)
                                elif isinstance(value, str) and '%' in value:
                                    sentiment_summary[clean_key] = float(value.replace('%', ''))
                                elif isinstance(value, (int, float)):
                                    sentiment_summary[clean_key] = float(value)
                                else:
                                    sentiment_summary[clean_key] = str(value)
                            except:
                                sentiment_summary[clean_key] = str(value)
            
            # Now find and process the event-by-event breakdown table
            event_header_row = None
            for idx in range(len(df)):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and 'Date' in str(row.iloc[0]) and pd.notna(row.iloc[1]) and 'Price' in str(row.iloc[1]):
                    event_header_row = idx
                    break
            
            events_df = pd.DataFrame()
            if event_header_row is not None:
                # Extract the events table
                events_df = df.iloc[event_header_row:].copy()
                
                # Set column names from the header row
                headers = []
                for col_idx in range(len(events_df.columns)):
                    if col_idx < len(events_df.iloc[0]) and pd.notna(events_df.iloc[0].iloc[col_idx]):
                        headers.append(str(events_df.iloc[0].iloc[col_idx]).strip())
                    else:
                        headers.append(f'Column_{col_idx}')
                
                events_df.columns = headers[:len(events_df.columns)]
                events_df = events_df.iloc[1:].reset_index(drop=True)
                
                # Filter out empty rows
                events_df = events_df.dropna(subset=['Date'])
                
                # Convert Date column
                if 'Date' in events_df.columns:
                    events_df['Date'] = pd.to_datetime(events_df['Date'], errors='coerce')
                    events_df = events_df.dropna(subset=['Date'])
                
                self.logger.info(f"Extracted {len(events_df)} sentiment events")
            
            # Combine summary and events
            if not events_df.empty:
                # Add summary statistics as metadata to events dataframe
                for key, value in sentiment_summary.items():
                    events_df[f'summary_{key}'] = value
                return events_df
            else:
                return pd.DataFrame([sentiment_summary]) if sentiment_summary else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Sentiment Analysis sheet: {e}")
            return pd.DataFrame()
    
    def _process_performance_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Performance Metrics sheet - ACTUALLY READ Excel values instead of hardcoding"""
        self.logger.info("Processing Performance Metrics sheet")
        
        try:
            performance_data = {}
            
            # Debug: Print the raw sheet to understand structure
            self.logger.info("Raw Performance sheet structure:")
            for idx in range(min(20, len(df))):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]):
                    self.logger.info(f"Row {idx}: '{row.iloc[0]}' | '{row.iloc[1] if len(row) > 1 else 'N/A'}'")
            
            # FIXED APPROACH: Actually read the Excel values instead of hardcoding
            for idx in range(len(df)):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and len(row) > 1 and pd.notna(row.iloc[1]):
                    period_str = str(row.iloc[0]).strip()
                    value_cell = row.iloc[1]
                    
                    # Check if this is a return period
                    if any(period in period_str for period in ['1 Day', '1 Week', '1 Month', '3 Month', '6 Month', '1 Year']):
                        
                        # Skip if value is N/A or similar
                        if value_cell == 'N/A' or pd.isna(value_cell):
                            continue
                        
                        try:
                            # CRITICAL FIX: Read the ACTUAL Excel value, don't hardcode!
                            percentage_value = None
                            
                            if isinstance(value_cell, str):
                                # If it's already a formatted string like "-18.00%"
                                if '%' in value_cell:
                                    percentage_value = float(value_cell.replace('%', '').strip())
                                else:
                                    # Try to parse as number
                                    percentage_value = float(value_cell)
                            elif isinstance(value_cell, (int, float)):
                                # Raw number from Excel
                                numeric_value = float(value_cell)
                                
                                # Determine if it's already a percentage or decimal
                                # If the absolute value is < 1, it's likely a decimal (0.06 = 6%)
                                # If the absolute value is >= 1, it's likely already a percentage (6.0 = 6%)
                                if abs(numeric_value) < 1:
                                    percentage_value = numeric_value * 100  # Convert decimal to percentage
                                else:
                                    percentage_value = numeric_value  # Already a percentage
                            
                            if percentage_value is not None:
                                # Create clean period key
                                clean_period = period_str.replace(':', '').strip().lower().replace(' ', '_')
                                
                                # Store the ACTUAL percentage value from Excel
                                performance_data[f'performance_{clean_period}'] = percentage_value
                                
                                # For 1-month specifically, create the aliases your app expects
                                if '1_month' in clean_period:
                                    performance_data['performance_return_1_month'] = percentage_value
                                    performance_data['return_1_month'] = percentage_value
                                    performance_data['monthly_return'] = percentage_value
                                    performance_data['30d_return'] = percentage_value
                                
                                self.logger.info(f"ACTUAL VALUE: Extracted {period_str}: {percentage_value}%")
                            
                        except Exception as e:
                            self.logger.warning(f"Error processing {period_str}: {e}")
            
            # Look for Risk Metrics (keep existing logic)
            for idx in range(len(df)):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and len(row) > 1:
                    metric = str(row.iloc[0]).strip()
                    
                    if any(term in metric for term in ['Annual Volatility', 'Sharpe Ratio', 
                                                    'Maximum Drawdown', 'Beta', 'Value at Risk']):
                        if pd.notna(row.iloc[1]) and row.iloc[1] != 'N/A':
                            value = row.iloc[1]
                            clean_key = metric.replace(' ', '_').replace(':', '').lower()
                            
                            try:
                                if isinstance(value, str) and '%' in value:
                                    performance_data[clean_key] = float(value.replace('%', ''))
                                elif isinstance(value, (int, float)):
                                    # Check if this is a percentage metric
                                    if 'volatility' in metric.lower() or 'drawdown' in metric.lower():
                                        performance_data[clean_key] = float(value) * 100
                                    else:
                                        performance_data[clean_key] = float(value)
                            except:
                                pass
            
            self.logger.info(f"ACTUAL VALUES: Final extracted performance metrics: {performance_data}")
            return pd.DataFrame([performance_data]) if performance_data else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Performance Metrics sheet: {e}")
            return pd.DataFrame()
    
    def _process_raw_data_sheet(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Raw Data sheet with OHLCV and event analysis"""
        self.logger.info("Processing Raw Data sheet")
        
        try:
            # Find header row
            header_row = None
            for idx in range(min(10, len(df))):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and 'Date' in str(row.iloc[0]):
                    header_row = idx
                    break
            
            if header_row is None:
                self.logger.warning("Could not find header row in Raw Data sheet")
                return pd.DataFrame()
            
            # Extract data
            raw_df = df.iloc[header_row:].copy()
            
            # Set column names from header row
            headers = []
            for col_idx in range(len(raw_df.columns)):
                if col_idx < len(raw_df.iloc[0]) and pd.notna(raw_df.iloc[0].iloc[col_idx]):
                    headers.append(str(raw_df.iloc[0].iloc[col_idx]).strip())
                else:
                    headers.append(f'Column_{col_idx}')
            
            raw_df.columns = headers[:len(raw_df.columns)]
            raw_df = raw_df.iloc[1:].reset_index(drop=True)
            
            # Clean and convert data
            raw_df = raw_df.dropna(subset=['Date'])
            
            # Convert Date
            raw_df['Date'] = pd.to_datetime(raw_df['Date'], errors='coerce')
            raw_df = raw_df.dropna(subset=['Date'])
            raw_df.set_index('Date', inplace=True)
            
            # Convert numeric columns
            numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'Daily Change %',
                             'Confidence', 'Impact', 'News Count', 'Sentiment_Confidence',
                             'Sentiment_Relevance']
            
            for col in numeric_columns:
                if col in raw_df.columns:
                    raw_df[col] = pd.to_numeric(raw_df[col], errors='coerce')
            
            # Sort by date
            raw_df = raw_df.sort_index()
            
            self.logger.info(f"Processed raw data with {len(raw_df)} rows and {len(raw_df.columns)} columns")
            return raw_df
            
        except Exception as e:
            self.logger.error(f"Error processing Raw Data sheet: {e}")
            return pd.DataFrame()
    
    def _process_data_quality_sheet(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Data Quality sheet"""
        self.logger.info("Processing Data Quality sheet")
        
        try:
            quality_metrics = {}
            
            # Extract overview metrics
            for idx in range(len(df)):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and len(row) > 1 and pd.notna(row.iloc[1]):
                    key = str(row.iloc[0]).strip()
                    value = row.iloc[1]
                    
                    # Skip section headers
                    if key in ['Data Overview', 'Missing Data Analysis', 'Data Integrity Checks', 
                               'Data Quality Summary']:
                        continue
                    
                    clean_key = key.replace(' ', '_').replace(':', '').lower()
                    
                    try:
                        if isinstance(value, str) and '%' in value:
                            quality_metrics[clean_key] = float(value.replace('%', ''))
                        elif isinstance(value, str) and value.isdigit():
                            quality_metrics[clean_key] = int(value)
                        elif isinstance(value, (int, float)):
                            quality_metrics[clean_key] = float(value)
                        else:
                            quality_metrics[clean_key] = str(value)
                    except:
                        quality_metrics[clean_key] = str(value)
            
            self.logger.info(f"Extracted {len(quality_metrics)} data quality metrics")
            return pd.DataFrame([quality_metrics]) if quality_metrics else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Data Quality sheet: {e}")
            return pd.DataFrame()
    
    def _process_metadata_sheet(self, df: pd.DataFrame) -> pd.DataFrame:
        """Process Metadata sheet"""
        self.logger.info("Processing Metadata sheet")
        
        try:
            metadata_info = {}
            
            for idx in range(len(df)):
                row = df.iloc[idx]
                if pd.notna(row.iloc[0]) and len(row) > 1 and pd.notna(row.iloc[1]):
                    key = str(row.iloc[0]).strip()
                    value = row.iloc[1]
                    
                    # Skip section headers
                    if key in ['Analysis Information', 'Technical Indicators Included', 
                               'Risk Metrics Calculated', 'Disclaimer']:
                        continue
                    
                    # Skip bullet points
                    if key.startswith('â€¢'):
                        continue
                    
                    clean_key = key.replace(' ', '_').replace(':', '').lower()
                    metadata_info[clean_key] = str(value)
            
            self.logger.info(f"Extracted {len(metadata_info)} metadata fields")
            return pd.DataFrame([metadata_info]) if metadata_info else pd.DataFrame()
            
        except Exception as e:
            self.logger.error(f"Error processing Metadata sheet: {e}")
            return pd.DataFrame()
    
    def get_enhanced_features_for_date(self, target_date: datetime, lookback_days: int = 30) -> Dict:
        """Enhanced feature extraction including ALL sheet data with FIXED 30-day return access"""
        target_date = pd.to_datetime(target_date)
        features = {}
        
        # 1. Raw data features (from Raw Data sheet)
        if self.raw_data is not None and not self.raw_data.empty:
            try:
                # Get the most recent data up to target date
                mask = self.raw_data.index <= target_date
                if mask.any():
                    latest_data = self.raw_data[mask].iloc[-1]
                    
                    features.update({
                        'close': float(latest_data.get('Close', 0)),
                        'volume': float(latest_data.get('Volume', 0)),
                        'daily_change': float(latest_data.get('Daily Change %', 0)),
                        'open': float(latest_data.get('Open', 0)),
                        'high': float(latest_data.get('High', 0)),
                        'low': float(latest_data.get('Low', 0))
                    })
                    
                    # Add event analysis features if available
                    if 'Event_Type' in latest_data.index and pd.notna(latest_data['Event_Type']):
                        features['event_type'] = str(latest_data['Event_Type'])
                        features['sentiment'] = str(latest_data.get('Sentiment', 'Neutral'))
                        features['confidence_score'] = float(latest_data.get('Confidence', 0))
                        features['impact_level'] = str(latest_data.get('Impact', 'LOW'))
                        features['news_count'] = int(latest_data.get('News Count', 0))
            except Exception as e:
                self.logger.warning(f"Error extracting raw data features: {e}")
        
        # 2. Technical indicator features
        if self.technical_data is not None and not self.technical_data.empty:
            try:
                mask = self.technical_data.index <= target_date
                if mask.any():
                    latest_tech = self.technical_data[mask].iloc[-1]
                    
                    tech_indicators = ['SMA_20', 'SMA_50', 'SMA_200', 'RSI', 'MACD', 
                                    'MACD_Signal', 'BB_Upper', 'BB_Lower', 'ATR']
                    
                    for indicator in tech_indicators:
                        if indicator in latest_tech.index and pd.notna(latest_tech[indicator]):
                            features[f'tech_{indicator.lower()}'] = float(latest_tech[indicator])
            except Exception as e:
                self.logger.warning(f"Error extracting technical features: {e}")
        
        # 3. Summary features (single row dataframe) - NO CONFLICTING ALIASES
        if self.summary_data is not None and not self.summary_data.empty:
            try:
                summary_row = self.summary_data.iloc[0]
                for col in self.summary_data.columns:
                    if pd.notna(summary_row[col]):
                        # Only include numeric values in features with summary_ prefix
                        if isinstance(summary_row[col], (int, float)):
                            features[f'summary_{col}'] = float(summary_row[col])
                            # DO NOT create any aliases that would conflict with performance data
            except Exception as e:
                self.logger.warning(f"Error extracting summary features: {e}")
        
        # 4. Company fundamental features
        if self.company_info is not None and not self.company_info.empty:
            try:
                company_row = self.company_info.iloc[0]
                fundamental_fields = ['market_cap', 'pe_ratio', 'forward_pe', 'peg_ratio',
                                    'price_to_sales', 'price_to_book', 'dividend_yield',
                                    'payout_ratio', 'beta', 'employees']
                
                for field in fundamental_fields:
                    if field in company_row.index and pd.notna(company_row[field]):
                        value = company_row[field]
                        if isinstance(value, (int, float)):
                            features[f'fundamental_{field}'] = float(value)
            except Exception as e:
                self.logger.warning(f"Error extracting company features: {e}")
        
        # 5. Performance features - COMPLETELY FIXED: Use only performance_1_month for aliases
        if self.performance_data is not None and not self.performance_data.empty:
            try:
                perf_row = self.performance_data.iloc[0]
                
                # First, add all performance data with performance_ prefix
                for col in self.performance_data.columns:
                    if pd.notna(perf_row[col]) and isinstance(perf_row[col], (int, float)):
                        features[f'performance_{col}'] = float(perf_row[col])
                
                # CRITICAL FIX: Only create 30-day aliases from performance_1_month
                if 'performance_1_month' in perf_row.index and pd.notna(perf_row['performance_1_month']):
                    correct_1_month_value = float(perf_row['performance_1_month'])  # This should be -18.0
                    
                    # Create ALL possible key names your app might use - ALL pointing to -18.0
                    features['performance_return_1_month'] = correct_1_month_value
                    features['return_1_month'] = correct_1_month_value
                    features['monthly_return'] = correct_1_month_value
                    features['30d_return'] = correct_1_month_value
                    features['30d_performance'] = correct_1_month_value
                    features['recent_30d_return'] = correct_1_month_value
                    features['one_month_return'] = correct_1_month_value
                    features['1m_return'] = correct_1_month_value
                    features['performance_1m'] = correct_1_month_value
                    
                    self.logger.info(f"FINAL FIX: Set ALL 30-day return keys to {correct_1_month_value}% from performance_1_month")
                    
            except Exception as e:
                self.logger.warning(f"Error extracting performance features: {e}")
        
        # 6. Sentiment features from sentiment analysis
        if self.sentiment_data is not None and not self.sentiment_data.empty:
            try:
                # If sentiment_data has Date column, get latest sentiment
                if 'Date' in self.sentiment_data.columns:
                    sentiment_df = self.sentiment_data.copy()
                    sentiment_df['Date'] = pd.to_datetime(sentiment_df['Date'], errors='coerce')
                    mask = sentiment_df['Date'] <= target_date
                    if mask.any():
                        latest_sentiment = sentiment_df[mask].iloc[-1]
                        
                        sentiment_fields = ['Sentiment', 'Conf.', 'News', 'Impact', 
                                        'Sent.Score', 'Method', 'Phase']
                        
                        for field in sentiment_fields:
                            if field in latest_sentiment.index and pd.notna(latest_sentiment[field]):
                                value = latest_sentiment[field]
                                if isinstance(value, str) and '%' in value:
                                    features[f'sentiment_{field.lower()}'] = float(value.replace('%', ''))
                                elif isinstance(value, (int, float)):
                                    features[f'sentiment_{field.lower()}'] = float(value)
                                else:
                                    features[f'sentiment_{field.lower()}'] = str(value)
                else:
                    # Summary statistics only
                    for col in self.sentiment_data.columns:
                        if 'summary_' in col and pd.notna(self.sentiment_data.iloc[0][col]):
                            value = self.sentiment_data.iloc[0][col]
                            if isinstance(value, (int, float)):
                                features[col] = float(value)
            except Exception as e:
                self.logger.warning(f"Error extracting sentiment features: {e}")
        
        # 7. Data quality score
        if self.data_quality is not None and not self.data_quality.empty:
            try:
                quality_row = self.data_quality.iloc[0]
                if 'data_completeness' in quality_row.index:
                    features['quality_completeness'] = float(quality_row['data_completeness'])
                if 'total_records' in quality_row.index:
                    features['quality_total_records'] = int(quality_row['total_records'])
            except Exception as e:
                self.logger.warning(f"Error extracting quality features: {e}")
        
        # 8. Time-based features
        features.update({
            'day_of_week': target_date.dayofweek,
            'month': target_date.month,
            'quarter': (target_date.month - 1) // 3 + 1,
            'year': target_date.year,
            'day_of_month': target_date.day
        })
        
        # 9. Calculate derived features
        if 'close' in features and 'open' in features and features['open'] > 0:
            features['intraday_change'] = (features['close'] - features['open']) / features['open'] * 100
        
        if 'high' in features and 'low' in features:
            features['price_range'] = features['high'] - features['low']
            if 'close' in features and features['low'] > 0:
                features['close_position'] = (features['close'] - features['low']) / (features['high'] - features['low']) if features['high'] > features['low'] else 0.5
        
        # Moving average signals
        if all(f'tech_sma_{period}' in features for period in [20, 50, 200]):
            if features['close'] > features['tech_sma_20']:
                features['above_sma_20'] = 1
            else:
                features['above_sma_20'] = 0
                
            if features['close'] > features['tech_sma_50']:
                features['above_sma_50'] = 1
            else:
                features['above_sma_50'] = 0
                
            if features['close'] > features['tech_sma_200']:
                features['above_sma_200'] = 1
            else:
                features['above_sma_200'] = 0
        
        self.logger.info(f"Extracted {len(features)} features for date {target_date.date()}")
        return features
    
    def get_comprehensive_summary(self) -> Dict:
        """Get comprehensive summary of all loaded data"""
        summary = {
            'data_sources_loaded': 0,
            'total_features_available': 0,
            'date_range': None,
            'data_quality_score': None,
            'company_info_available': False,
            'sentiment_events_analyzed': 0,
            'technical_indicators_available': [],
            'performance_metrics_available': [],
            'recommendations': []
        }
        
        # Count loaded data sources
        sources = {
            'raw_data': self.raw_data,
            'technical_data': self.technical_data,
            'sentiment_data': self.sentiment_data,
            'performance_data': self.performance_data,
            'summary_data': self.summary_data,
            'company_info': self.company_info,
            'data_quality': self.data_quality,
            'metadata': self.metadata,
            'price_charts_info': self.price_charts_info
        }
        
        for name, source in sources.items():
            if source is not None and (isinstance(source, pd.DataFrame) and not source.empty):
                summary['data_sources_loaded'] += 1
        
        # Get date range
        date_sources = []
        if self.raw_data is not None and not self.raw_data.empty and isinstance(self.raw_data.index, pd.DatetimeIndex):
            date_sources.append(self.raw_data.index)
        if self.technical_data is not None and not self.technical_data.empty and isinstance(self.technical_data.index, pd.DatetimeIndex):
            date_sources.append(self.technical_data.index)
        
        if date_sources:
            all_dates = pd.concat([ds.to_series() for ds in date_sources])
            summary['date_range'] = f"{all_dates.min().date()} to {all_dates.max().date()}"
        
        # Get latest feature count
        try:
            if self.raw_data is not None and not self.raw_data.empty:
                latest_date = self.raw_data.index.max()
                features = self.get_enhanced_features_for_date(latest_date)
                summary['total_features_available'] = len(features)
        except:
            summary['total_features_available'] = 0
        
        # Company info
        summary['company_info_available'] = self.company_info is not None and not self.company_info.empty
        
        # Sentiment events
        if self.sentiment_data is not None and not self.sentiment_data.empty:
            if 'Date' in self.sentiment_data.columns:
                summary['sentiment_events_analyzed'] = len(self.sentiment_data)
            else:
                # Look for summary statistic
                if 'summary_total_analyzed_events' in self.sentiment_data.columns:
                    summary['sentiment_events_analyzed'] = int(self.sentiment_data.iloc[0]['summary_total_analyzed_events'])
        
        # Technical indicators available
        if self.technical_data is not None and not self.technical_data.empty:
            tech_indicators = [col for col in self.technical_data.columns 
                             if col not in ['Date', 'Close', 'Volume']]
            summary['technical_indicators_available'] = tech_indicators
        
        # Performance metrics available
        if self.performance_data is not None and not self.performance_data.empty:
            perf_metrics = list(self.performance_data.columns)
            summary['performance_metrics_available'] = perf_metrics
        
        # Data quality score
        if self.data_quality is not None and not self.data_quality.empty:
            if 'data_completeness' in self.data_quality.columns:
                summary['data_quality_score'] = self.data_quality.iloc[0]['data_completeness']
        
        # Generate recommendations
        if summary['data_sources_loaded'] < 5:
            summary['recommendations'].append(f"Only {summary['data_sources_loaded']} data sources loaded. Check Excel file structure.")
        
        if summary['total_features_available'] < 20:
            summary['recommendations'].append("Limited features available. Ensure all sheets are properly formatted.")
        
        if not summary['company_info_available']:
            summary['recommendations'].append("Company information not available. Check Company Info sheet.")
        
        if summary['sentiment_events_analyzed'] == 0:
            summary['recommendations'].append("No sentiment events found. Run sentiment analysis on the data.")
        
        return summary
    
    def get_all_dates_with_events(self) -> List[datetime]:
        """Get all dates that have significant events"""
        event_dates = []
        
        if self.raw_data is not None and not self.raw_data.empty:
            # Check for event columns
            if 'Event_Type' in self.raw_data.columns:
                # Get dates where Event_Type is not empty
                event_mask = self.raw_data['Event_Type'].notna() & (self.raw_data['Event_Type'] != '')
                event_dates.extend(self.raw_data[event_mask].index.tolist())
        
        if self.sentiment_data is not None and not self.sentiment_data.empty:
            if 'Date' in self.sentiment_data.columns:
                # Convert dates and add to list
                sentiment_dates = pd.to_datetime(self.sentiment_data['Date'], errors='coerce')
                valid_dates = sentiment_dates.dropna()
                event_dates.extend(valid_dates.tolist())
        
        # Remove duplicates and sort
        unique_dates = list(set(event_dates))
        unique_dates.sort(reverse=True)  # Most recent first
        
        return unique_dates
    
    def get_ticker_symbol(self) -> str:
        """Extract ticker symbol from the data"""
        # Try from metadata
        if self.metadata is not None and not self.metadata.empty:
            if 'ticker_symbol' in self.metadata.columns:
                return str(self.metadata.iloc[0]['ticker_symbol'])
        
        # Try from summary
        if self.summary_data is not None and not self.summary_data.empty:
            for col in self.summary_data.columns:
                if 'symbol' in col.lower() or 'ticker' in col.lower():
                    return str(self.summary_data.iloc[0][col])
        
        # Try to extract from filename
        if hasattr(self, 'excel_path') and self.excel_path:
            import re
            filename = Path(self.excel_path).stem
            # Look for common ticker patterns (e.g., MSFT_analysis_report)
            match = re.match(r'^([A-Z]+)_', filename)
            if match:
                return match.group(1)
        
        return "UNKNOWN"
    
    def validate_data_integrity(self) -> Dict[str, Any]:
        """Validate the integrity of loaded data"""
        validation_results = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'data_coverage': {}
        }
        
        # Check each data source
        if self.raw_data is None or self.raw_data.empty:
            validation_results['errors'].append("Raw data is missing or empty")
            validation_results['valid'] = False
        else:
            validation_results['data_coverage']['raw_data'] = len(self.raw_data)
        
        if self.technical_data is None or self.technical_data.empty:
            validation_results['warnings'].append("Technical analysis data is missing")
        else:
            validation_results['data_coverage']['technical_data'] = len(self.technical_data)
        
        if self.summary_data is None or self.summary_data.empty:
            validation_results['warnings'].append("Summary data is missing")
        
        # Check date alignment between raw and technical data
        if (self.raw_data is not None and not self.raw_data.empty and 
            self.technical_data is not None and not self.technical_data.empty):
            
            raw_dates = set(self.raw_data.index)
            tech_dates = set(self.technical_data.index)
            
            missing_in_tech = raw_dates - tech_dates
            missing_in_raw = tech_dates - raw_dates
            
            if missing_in_tech:
                validation_results['warnings'].append(
                    f"{len(missing_in_tech)} dates in raw data not found in technical data"
                )
            
            if missing_in_raw:
                validation_results['warnings'].append(
                    f"{len(missing_in_raw)} dates in technical data not found in raw data"
                )
        
        return validation_results
    
    def debug_performance_data(self):
        """Enhanced debug method to verify performance data is loaded correctly"""
        print("\n" + "="*60)
        print("DEBUG: Performance Data Verification - ENHANCED")
        print("="*60)
        
        # Check performance_data DataFrame
        if self.performance_data is not None and not self.performance_data.empty:
            print("\n1. Performance Data DataFrame:")
            print(f"   Shape: {self.performance_data.shape}")
            print(f"   Columns: {list(self.performance_data.columns)}")
            print("\n   Values:")
            for col in self.performance_data.columns:
                value = self.performance_data.iloc[0][col]
                print(f"   {col}: {value}")
        else:
            print("\nâš  No performance data loaded!")
        
        # Check what features are extracted
        print("\n2. Extracted Features (for today):")
        features = self.get_enhanced_features_for_date(datetime.now())
        
        # Look for all performance-related features
        perf_features = {k: v for k, v in features.items() 
                        if any(term in k.lower() for term in ['performance', 'return', 'month', '30d'])}
        
        if perf_features:
            print(f"   Found {len(perf_features)} performance features:")
            for key, value in perf_features.items():
                print(f"   {key}: {value}")
        else:
            print("   âš  No performance features found!")
        
        # Check summary data for comparison
        print("\n3. Summary Data (1-month return):")
        if self.summary_data is not None and not self.summary_data.empty:
            for col in self.summary_data.columns:
                if any(term in col.lower() for term in ['month', '30']):
                    value = self.summary_data.iloc[0][col]
                    print(f"   {col}: {value}")
        
        # CRITICAL: Test all possible key names your app might use
        print("\n4. CRITICAL: Testing Key Names Your App Might Use:")
        test_keys = [
            '30d_performance',
            'recent_30d_return', 
            'performance_return_1_month',
            'return_1_month',
            'monthly_return',
            '30d_return',
            'one_month_return',
            '1m_return',
            'performance_1m',
            'summary_1_month',
            'summary_monthly_return'
        ]
        
        found_keys = []
        for key in test_keys:
            if key in features:
                found_keys.append(f"âœ“ {key}: {features[key]}%")
            else:
                found_keys.append(f"âœ— {key}: NOT FOUND")
        
        for result in found_keys:
            print(f"   {result}")
        
        # Show what your app should use
        print("\n5. RECOMMENDED FIX FOR YOUR MAIN APP:")
        if any('âœ“' in result for result in found_keys):
            working_key = next((key for key in test_keys if key in features), None)
            if working_key:
                print(f"   Replace this in your main app:")
                print(f"   OLD: performance = features.get('30d_performance', 0.01)")
                print(f"   NEW: performance = features.get('{working_key}', 0.0)")
                print(f"   This will show: Recent 30d performance: +{features[working_key]:.2f}%")
        
        print("\n" + "="*60)
    
    def get_30d_performance(self) -> float:
        """Direct method to get 30-day performance - NO CONFUSION POSSIBLE"""
        features = self.get_enhanced_features_for_date(datetime.now())
        
        # Try all possible keys in order of preference
        possible_keys = [
            'performance_return_1_month',
            'return_1_month', 
            'monthly_return',
            '30d_return',
            'performance_1m',
            'summary_1_month',
            '30d_performance',
            'recent_30d_return'
        ]
        
        for key in possible_keys:
            if key in features and isinstance(features[key], (int, float)):
                self.logger.info(f"Found 30-day performance using key '{key}': {features[key]}%")
                return float(features[key])
        
        # If no performance data found, log warning
        self.logger.warning("No 30-day performance data found in any expected keys!")
        return 0.0


# Test usage with enhanced debugging
if __name__ == "__main__":
    loader = ExcelDataLoader()
    all_data = loader.load_all_data()
    
    # Enhanced debugging
    loader.debug_performance_data()
    
    # Test the direct method
    print(f"\nDirect 30-day performance: {loader.get_30d_performance()}%")
    
    # Show exactly what your main app should use
    print("\n" + "="*80)
    print("SOLUTION FOR YOUR MAIN APPLICATION:")
    print("="*80)
    print("Replace the problematic line in your main app with:")
    print()
    print("# OLD (causing 0.01% display):")
    print("# performance = features.get('30d_performance', 0.01)")
    print()
    print("# NEW (will show correct 6.19%):")
    print("performance = loader.get_30d_performance()")
    print("# OR:")
    print("performance = features.get('performance_return_1_month', 0.0)")
    print()
    print("This will display: 'Recent 30d performance: +6.19%' instead of '+0.01%'")
    print("="*80)